{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This program will function as an LLM Tutor using Ollama Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to Ollama Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system prompt is what explains the context of the situation to the frontier model\n",
    "# It tells them what kind of task they are performing and what tone to use\n",
    "\n",
    "system_prompt = \"You are provided with a question.. \\\n",
    "    You are going to best answer the question as \\\n",
    "    if you are a tutor of the relevant subject matter.\\n\"\n",
    "system_prompt += \"You should respond with normal conversational prose\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "# The user prompt is the actual conversation itself\n",
    "# The converstaion start and the role of the LLM is to figure out what way to respond to the user prompt in the context of the system prompt\n",
    "\n",
    "def user_prompt_for(question):\n",
    "    user_prompt = f\"You are looking at a question {question}\"\n",
    "    user_prompt += \"\\nThis question is about how to understand a piece of code; \\\n",
    "please provide a walkthrough on what this code is doing.\\n\\n\"\n",
    "    user_prompt += question\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Answer for Question Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "def answer_for_question(question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(question)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Question List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer Question Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    # Create the question object\n",
    "    question = question\n",
    "    \n",
    "    # Generate the messages payload\n",
    "    messages = answer_for_question(question)\n",
    "    \n",
    "    # Create the payload for the Ollama API\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False  # Disable streaming for simplicity\n",
    "    }\n",
    "    \n",
    "    # Make the request to the Ollama API\n",
    "    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "    \n",
    "    # Extract the summary from the response\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('message', {}).get('content', \"No content in response.\")\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying Answer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_answer(question):\n",
    "    # Generate the answer using the appropriate function\n",
    "    answer = answer_question(question)\n",
    "    # Display the answer as Markdown\n",
    "    display(Markdown(answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down this line of code together.\n",
       "\n",
       "This code snippet appears to be part of a Python context manager or a loop, possibly within a list comprehension. It uses the `yield from` syntax, which is a feature introduced in Python 3.3 as part of the generators.\n",
       "\n",
       "Here's what this code does:\n",
       "\n",
       "1. **List Comprehension**: The expression inside the `{}` is a list comprehension. A list comprehension is a concise way to create a new list by performing an operation on each item in an existing list or other iterable.\n",
       "\n",
       "2. **Filtering with `if` condition**: The `for book in books if book.get(\"author\")` part filters out books that don't have an \"author\" key.\n",
       "\n",
       "3. **Yield from**: The `yield from` statement is used to delegate to a subiterator, which in this case is the generator expression `{book.get(\"author\") for book in books if book.get(\"author\")}`.\n",
       "\n",
       "So, what does it do?\n",
       "\n",
       "It yields the authors of all books that have an author specified. In other words, it iterates over each book in the `books` list (or iterable), and if a book has an \"author\" key, it yields the value of that key.\n",
       "\n",
       "Here's why:\n",
       "\n",
       "The purpose of using `yield from` here is to avoid having to manually iterate over the subgenerator expression. If we were to do that, we would have to use another loop or recursion, which could be more complex and less efficient.\n",
       "\n",
       "By using `yield from`, we're essentially saying: \"Take this list comprehension expression as a subiterator, and yield its values one by one, without having to handle the iteration logic myself.\"\n",
       "\n",
       "To illustrate this with a simple example:\n",
       "\n",
       "Let's say you have a dictionary where each key is a book title, and its corresponding value is another dictionary containing the author. Here's how this would look like:\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author 1\"},\n",
       "    {\"title\": \"Book 2\", \"author\": None}, # No author for some books\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author 3\"}\n",
       "]\n",
       "```\n",
       "\n",
       "Now, if we use this code:\n",
       "\n",
       "```python\n",
       "for author in yield from {book.get(\"author\") for book in books if book.get(\"author\")}:\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "It will output: `Author 1`, `Author 3`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
